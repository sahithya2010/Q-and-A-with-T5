# -*- coding: utf-8 -*-
"""poeple_basic_train_qa_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ObT0j3fIRdE56vvMAEZUcORrvXBdkeUA

https://www.kaggle.com/code/abdokamr/question-answering-with-t5
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import scipy.spatial
import matplotlib.pyplot as plt


#from transformers import T5Tokenizer, T5ForConditionalGeneration
from tqdm import tqdm
import pandas as pd
from torch.utils.data import DataLoader, SequentialSampler, TensorDataset
import torch
import pandas as pd
from pprint import pprint

import sys
import os
import glob

!apt install git-lfs

!pip install transformers sentencepiece datasets evaluate accelerate

!pip install --upgrade simplet5

from datasets import load_dataset
raw_dataset=load_dataset("sahithya20/famous_people_1")

raw_dataset



data=pd.read_csv("/content/people_100.csv", encoding='utf-8')
data

df= data.drop(['answer_start'], axis=1)

df.rename(columns={'Description':'context'}, inplace=True)

df.rename(columns={'Answer':'line'}, inplace=True)

df

df=pd.read_csv("/content/sample_doc.csv", encoding='latin-1')
df

import pandas as pd


# Create lists to store source_text (questions) and target_text (answers)
source_text = []
target_text = []

# Iterate over each row in the DataFrame
for _, row in df.iterrows():
    for col in df.columns:
        # Use column names as questions and row values as answers
        source_text.append(col)
        target_text.append(str(row[col]) + " </s>")  # Ensure values are strings and add the separator

# Create a dictionary with the processed data
dict_data = {
    'source_text': source_text,
    'target_text': target_text
}

# Convert the dictionary into a DataFrame
processed_df = pd.DataFrame(dict_data)

q_list = df['Question']                          # questions list to feed the model
n_list = df['context'] + " </s>"  # answers list to feed the model

dict_data = {'source_text': q_list,
      'target_text': n_list}

df = pd.DataFrame(dict_data)
df.head()

processed_df.head()

processed_df['source_text'][10]

processed_df['target_text'][10]

train_data, val_data = train_test_split(df[:-12], test_size=0.2)
test_data = df[-5:]
train_data.shape, val_data.shape, test_data.shape

train_data
val_data

from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration



# Commented out IPython magic to ensure Python compatibility.
# %%time
# from simplet5 import SimpleT5
# 
# model = SimpleT5()
# model.from_pretrained(model_type="t5", model_name="t5-small")
# model.train(train_df = train_data,
#             eval_df = val_data,
#             source_max_token_len=128,
#             target_max_token_len=100,
#             batch_size=4, max_epochs=3, use_gpu=True)

ls ./outputs

# let's load the trained model for inferencing

model.load_model("t5","./outputs/simplet5-epoch-0-train-loss-3.3914-val-loss-2.4761/", use_gpu=True)

test_data['source_text']

q_test = test_data['source_text'][2222]
q_ans = test_data['target_text'][2222]

print("Question: ", q_test)
print('-'*50)
print("Answer: ",q_ans)



from huggingface_hub import notebook_login

notebook_login()

!pip install huggingface_hub()

from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoTokenizer, AutoModelForQuestionAnswering

!rm -r {model_dir}

batch_size = 4
model_name="train_1"
output_dir="experiments"
args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    evaluation_strategy="steps",
    eval_steps=100,
    logging_strategy="steps",
    logging_steps=100,
    save_strategy="steps",
    save_steps=200,
    learning_rate=4e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    load_best_model_at_end=True,
    push_to_hub=True
)

from transformers import T5Tokenizer
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("t5-small")

data_collator = DataCollatorForSeq2Seq(tokenizer)

# Function that returns an untrained model to be trained
def model_init():
    return AutoModelForSeq2SeqLM.from_pretrained("t5-small")

trainer = Seq2SeqTrainer(
    model_init=model_init,
    args=args,
    train_dataset=train_data,
    eval_dataset=val_data,
    data_collator=data_collator,
    tokenizer=tokenizer
)

!git lfs install --skip-repo

trainer.train()

trainer.save_model()

from huggingface_hub import HfApi
api = HfApi()

api.upload_folder(
    folder_path="/content/outputs/simplet5-epoch-0-train-loss-3.3914-val-loss-2.4761",
    repo_id="sahithya20/experiments",
    repo_type="model",
)